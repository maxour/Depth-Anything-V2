# åˆ‡æ¢åˆ°ç‹¬ç«‹ç¯å¢ƒ
# python3 -m venv venv
# source venv/bin/activate
# pip3 install -r requirements.txt
import cv2
import torch
import numpy as np
import os
import argparse
from depth_anything_v2.dpt import DepthAnythingV2

# === é…ç½® ===
DEVICE = 'mps' if torch.backends.mps.is_available() else 'cpu'
# è¿™é‡Œçš„ encoder å¿…é¡»å’Œä½ ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶åä¸€è‡´ (vits, vitb, vitl)
model_configs = {
        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
        'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
    }

def extract_and_process(video_path, frame_index, output_dir, encoder):
    # 1. åˆå§‹åŒ–æ¨¡å‹
    print(f"ğŸš€ Loading model to {DEVICE}...")
    depth_model = DepthAnythingV2(**model_configs[encoder])
    depth_model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_{encoder}.pth', map_location='cpu'))
    depth_model = depth_model.to(DEVICE).eval()

    # 2. è¯»å–è§†é¢‘æŒ‡å®šå¸§
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if frame_index >= total_frames:
        print(f"âŒ Error: Frame {frame_index} out of bounds (Total: {total_frames})")
        return

    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
    ret, frame = cap.read()
    cap.release()

    if not ret:
        print("âŒ Error: Could not read frame.")
        return

    h, w = frame.shape[:2]
    print(f"ğŸ“¸ Captured Frame {frame_index} | Resolution: {w}x{h}")

    # 3. å¤„ç†æ¥ç¼ (The Padding Trick)
    # å·¦å³å„æ‰©å…… 10% çš„å†…å®¹
    pad_w = int(w * 0.1) 
    left_part = frame[:, 0:pad_w] 
    right_part = frame[:, w-pad_w:w] 
    padded_frame = np.concatenate((right_part, frame, left_part), axis=1)

    # === å…³é”®ä¿®æ”¹å¼€å§‹ ===
    
    # è·å– Padding åçš„åŸå§‹å°ºå¯¸
    orig_pad_h, orig_pad_w = padded_frame.shape[:2]

    # [æ­¥éª¤ A] æ‰‹åŠ¨ç¼©æ”¾ (Force Resize)
    # å¼ºåˆ¶å°†å›¾ç‰‡ç¼©å°åˆ°æ¨¡å‹èƒ½å¤„ç†çš„å¤§å° (ä¾‹å¦‚å®½ 1024 æˆ– 1176 - æœ€å¥½æ˜¯14çš„å€æ•°)
    # å¯¹äº Mac Airï¼Œæ¨èä½¿ç”¨ 1024 ä»¥ä¿è¯ä¸çˆ†æ˜¾å­˜
    infer_width = 1024 
    ratio = infer_width / orig_pad_w
    infer_height = int(orig_pad_h * ratio)
    
    # ç¡®ä¿é«˜åº¦æ˜¯ 14 çš„å€æ•° (ViT æ¨¡å‹å¯¹ Patch å¯¹é½æœ‰è¦æ±‚ï¼Œè™½ç„¶åº“é€šå¸¸ä¼šå¤„ç†ï¼Œä½†æ‰‹åŠ¨åšæ›´ç¨³)
    infer_height = (infer_height // 14) * 14
    infer_width = (infer_width // 14) * 14
    
    resized_padded_frame = cv2.resize(padded_frame, (infer_width, infer_height))

    print(f"ğŸ“‰ Resizing for inference: {orig_pad_w}x{orig_pad_h} -> {infer_width}x{infer_height}")

    # 4. æ·±åº¦æ¨ç†
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¼ å…¥å·²ç»ç¼©å°çš„å›¾ç‰‡ï¼Œinput_size å‚æ•°å¯ä»¥çœç•¥æˆ–ä¿æŒä¸€è‡´
    with torch.no_grad():
        depth = depth_model.infer_image(resized_padded_frame, input_size=infer_width)

    # 5. åå¤„ç†ä¸è£åˆ‡
    # [æ­¥éª¤ B] æ¢å¤å°ºå¯¸ (Upscale back)
    # å°†ç”Ÿæˆçš„ä½åˆ†è¾¨ç‡æ·±åº¦å›¾æ”¾å¤§å› padded çš„åŸå§‹å°ºå¯¸
    depth = cv2.resize(depth, (orig_pad_w, orig_pad_h), interpolation=cv2.INTER_CUBIC)

    # å½’ä¸€åŒ–åˆ° 0-255
    depth_normalized = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0
    depth_uint8 = depth_normalized.astype(np.uint8)

    # æŠŠä¹‹å‰æ‰©å……çš„ 10% åˆ‡æ‰ï¼Œåªä¿ç•™ä¸­é—´åŸæœ¬çš„éƒ¨åˆ†
    # æ­¤æ—¶ depth_uint8 çš„å°ºå¯¸å·²ç»å›åˆ°äº† orig_pad_w x orig_pad_h
    final_depth = depth_uint8[:, pad_w : orig_pad_w - pad_w]
    
    # [æ­¥éª¤ C] åŒé‡ä¿é™©ï¼šç¡®ä¿æœ€ç»ˆå°ºå¯¸ä¸¥æ ¼åŒ¹é…åŸè§†é¢‘å¸§
    final_depth = cv2.resize(final_depth, (w, h))

    # 6. ä¿å­˜ç»“æœ
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    name = os.path.splitext(os.path.basename(video_path))[0]
    
    # ä¿å­˜åŸå›¾ (ä½œä¸ºçº¹ç†)
    cv2.imwrite(os.path.join(output_dir, f"{name}_f{frame_index}_RGB.jpg"), frame)
    
    # ä¿å­˜æ·±åº¦å›¾ (ä½œä¸º Displacement/Depth)
    cv2.imwrite(os.path.join(output_dir, f"{name}_f{frame_index}_Depth.png"), final_depth)
    
    # ä¿å­˜åˆæˆé¢„è§ˆ (ä¸Šä¸‹æ’åˆ—)
    depth_color = cv2.cvtColor(final_depth, cv2.COLOR_GRAY2BGR)
    preview = np.vstack((frame, depth_color))
    cv2.imwrite(os.path.join(output_dir, f"{name}_f{frame_index}_Preview.jpg"), preview)

    print(f"âœ… Done! Files saved in {output_dir}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--video', type=str, required=True, help='Path to insv/mp4 video')
    parser.add_argument('--frame', type=int, default=0, help='Frame index to extract')
    parser.add_argument('--out', type=str, default='./out', help='Output folder')
    parser.add_argument('--encoder', type=str, default='vitl', choices=['vits', 'vitb', 'vitl', 'vitg'])
    
    args = parser.parse_args()
    extract_and_process(args.video, args.frame, args.out, args.encoder)